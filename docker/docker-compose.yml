# OJT Master - Docker Compose Configuration (Issue #104)
# Frontend Only - 외부 Local AI 서버 사용
#
# ⚠️ 중요: vLLM은 별도 서버(ojt-local-ai)에서 실행 중입니다.
#    이 docker-compose에서는 vLLM을 실행하지 않습니다.
#    AI 서버: http://10.10.100.209:8001 (ojt-local-ai 컨테이너)
#
# 사용법:
#   1. SSL 인증서 설정: docker/ssl/cert.pem, docker/ssl/key.pem
#   2. 프론트엔드 빌드: cd src-vite && npm run build
#   3. 실행: docker-compose up -d
#
# 구조:
#   nginx:8443 (HTTPS) ── / → frontend (정적 파일)
#   프론트엔드 → VITE_LOCAL_AI_URL → 외부 AI 서버 (10.10.100.209:8001)

services:
  # Nginx: SSL termination + Static File Server
  nginx:
    image: nginx:alpine
    container_name: ojt-nginx
    ports:
      - "8080:80"
      - "8443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ../src-vite/dist:/usr/share/nginx/html:ro
      - ./ssl:/etc/nginx/ssl:ro
    networks:
      - ojt-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  ojt-network:
    driver: bridge
    name: ojt-network

# ============================================================
# ⚠️ vLLM 서비스는 별도로 관리됩니다 (ojt-local-ai 컨테이너)
# ============================================================
#
# 이 프로젝트에서는 vLLM을 docker-compose로 실행하지 않습니다.
# 기존에 실행 중인 ojt-local-ai 컨테이너를 사용합니다.
#
# AI 서버 상태 확인:
#   docker ps | grep ojt-local-ai
#   curl http://10.10.100.209:8001/health
#
# AI 서버가 없는 경우 별도로 실행:
#   docker run -d --name ojt-local-ai \
#     --gpus all \
#     -p 8001:8000 \
#     vllm/vllm-openai:latest \
#     --model Qwen/Qwen3-4B \
#     --host 0.0.0.0 \
#     --port 8000 \
#     --max-model-len 4096 \
#     --gpu-memory-utilization 0.7 \
#     --enforce-eager
#
# ============================================================
